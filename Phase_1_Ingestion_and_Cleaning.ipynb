{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7908f475-93f0-46bd-a162-0e4a2d68fa4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# List of paths for the large monthly trip data files (Parquet).\n",
    "# Since only the month changes, we can use a list comprehension for efficiency.\n",
    "TRIP_DATA_PATHS = [\n",
    "    f\"/Volumes/nyc_yellow_tripdata/default/filestore/yellow_tripdata_2025-{month:02d}.parquet\" \n",
    "    for month in range(1, 9) # This generates months 01-08\n",
    "]\n",
    "\n",
    "# Path for the Taxi Zone Lookup CSV\n",
    "LOOKUP_TABLE_PATH = \"/Volumes/nyc_yellow_tripdata/default/filestore/taxi_zone_lookup.csv\"\n",
    "\n",
    "\n",
    "# --- Ingestion: Read and Union Trip Data ---\n",
    "\n",
    "print(\"Starting ingestion and union of 8 months of trip data...\")\n",
    "\n",
    "\n",
    "raw_trips_df = (spark.read\n",
    "                .format(\"parquet\")\n",
    "                .load(TRIP_DATA_PATHS)\n",
    "               )\n",
    "\n",
    "# Verifying the load by counting and displaying a sample\n",
    "total_records = raw_trips_df.count()\n",
    "print(f\"Total raw records loaded: {total_records}\")\n",
    "\n",
    "print(\"\\nRaw Data Schema:\")\n",
    "# Print the data types to ensure they were inferred correctly\n",
    "raw_trips_df.printSchema()\n",
    "\n",
    "print(\"\\nRaw Data Sample:\")\n",
    "# .display() is the Databricks-specific command to show a nicely formatted, interactive table\n",
    "raw_trips_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721563ed-ed32-4d42-8710-4898de41a07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting initial cleaning and filtering...\")\n",
    "\n",
    "# Filtering out records with invalid or impossible values ensures the integrity of all subsequent analytical calculations (e.g., averages, sums, and ML model training)\n",
    "\n",
    "cleaned_trips_df = (raw_trips_df\n",
    "    # TRIP DISTANCE FILTER\n",
    "    # A trip distance of 0 is either an error or a canceled ride where the meter was not reset.\n",
    "    # We must exclude these as they create infinite speed values (distance/time) and skew metrics.\n",
    "    .filter(col(\"trip_distance\") > 0)\n",
    "    \n",
    "    # FARE AMOUNT FILTER\n",
    "    # A fare of less than $1 suggests a free trip, a severe error, or a test transaction.\n",
    "    # Filtering these helps stabilize financial metrics like Average Fare.\n",
    "    .filter(col(\"fare_amount\") >= 1.0)\n",
    "    \n",
    "    # PASSENGER COUNT FILTER\n",
    "    # A passenger count of 0 is illogical for a completed trip and usually indicates a data entry error.\n",
    "    .filter(col(\"passenger_count\") > 0)\n",
    "    \n",
    "    # 4. PAYMENT TYPE FILTER\n",
    "    # Focusing on payment types 1 (Credit Card) and 2 (Cash) provides the most reliable data\n",
    "    # for revenue analysis. Types 3 (No Charge) and 4 (Dispute) are typically excluded from core revenue KPIs.\n",
    "    .filter(col(\"payment_type\").isin([1, 2])) \n",
    "    \n",
    "    # COLUMN RENAMING\n",
    "    # Renaming columns to a cleaner, more generalized standard makes the code easier to read\n",
    "    # and maintain, as 'tpep_' prefixes are specific to \"Yellow Taxi\" data.\n",
    "    .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "    .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    ")\n",
    "\n",
    "# Check removed rows.\n",
    "initial_records = raw_trips_df.count()\n",
    "final_records = cleaned_trips_df.count()\n",
    "print(f\"Initial Records: {initial_records}\")\n",
    "print(f\"Records after cleaning: {final_records}\")\n",
    "print(f\"Records removed: {initial_records - final_records}\")\n",
    "\n",
    "# Display schema to see the renamed columns\n",
    "print(\"\\nCleaned Data Schema:\")\n",
    "cleaned_trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2445aa-d946-4841-a962-ebaf7ab4c805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting feature engineering...\")\n",
    "\n",
    "\n",
    "transformed_df = (cleaned_trips_df\n",
    "    # TRIP DURATION CALCULATION\n",
    "    # Feature: 'trip_duration_seconds'\n",
    "    # This feature is essential for time-based analysis (e.g., identifying high-traffic hours) and is necessary to calculate speed. We use the PySpark `unix_timestamp` function for fast, distributed calculation.\n",
    "    .withColumn(\n",
    "        \"trip_duration_seconds\",\n",
    "        unix_timestamp(col(\"dropoff_datetime\")) - unix_timestamp(col(\"pickup_datetime\"))\n",
    "    )\n",
    "    \n",
    "    # AVERAGE SPEED CALCULATION\n",
    "    # Feature: 'avg_speed_mph'\n",
    "    # Average speed is a powerful metric for analyzing city congestion, driver efficiency, and identifying potential outliers (like trips with impossible speeds).\n",
    "    .withColumn(\n",
    "        \"avg_speed_mph\",\n",
    "        # Formula: distance / (duration_in_seconds / 3600 seconds per hour)\n",
    "        col(\"trip_distance\") / (col(\"trip_duration_seconds\") / 3600)\n",
    "    )\n",
    "    \n",
    "    #  FINAL DURATION FILTER\n",
    "    # A trip lasting less than 60 seconds is physically unlikely for the NYC system. \n",
    "    # This filter removes more noise, ensuring we focus on legitimate completed trips.\n",
    "    .filter(col(\"trip_duration_seconds\") > 60) \n",
    "    \n",
    "    # FINAL SPEED FILTER\n",
    "    # Speeds over 60 MPH are nearly impossible within city limits. This acts as a final layer of quality control, identifying and removing trips that are likely GPS errors or data corruption.\n",
    "    .filter(col(\"avg_speed_mph\") <= 60) \n",
    "    \n",
    "    # SELECT AND ORDER COLUMNS\n",
    "    # RExplicitly selecting columns ensures only relevant fields are carried forward, improving performance (by reducing data size) and providing a clean output schema for the next phase.\n",
    "    .select(\n",
    "        \"VendorID\", \"pickup_datetime\", \"dropoff_datetime\", \n",
    "        \"PULocationID\", \"DOLocationID\", \"passenger_count\",\n",
    "        \"trip_distance\", \"fare_amount\", \"tip_amount\", \n",
    "        \"total_amount\", \"payment_type\", \n",
    "        \"trip_duration_seconds\", \"avg_speed_mph\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Final verification of the total count after all filters.\n",
    "print(f\"Final records after feature engineering and final speed filter: {transformed_df.count()}\")\n",
    "transformed_df.limit(5).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase_1_Ingestion_and_Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7908f475-93f0-46bd-a162-0e4a2d68fa4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# List of paths for the large monthly trip data files (Parquet).\n",
    "TRIP_DATA_PATHS = [\n",
    "    f\"/Volumes/nyc_yellow_tripdata/default/filestore/yellow_tripdata_2025-{month:02d}.parquet\" \n",
    "    for month in range(1, 9) # This generates months 01-08\n",
    "]\n",
    "\n",
    "# Path for the Taxi Zone Lookup CSV\n",
    "LOOKUP_TABLE_PATH = \"/Volumes/nyc_yellow_tripdata/default/filestore/taxi_zone_lookup.csv\"\n",
    "\n",
    "\n",
    "# --- Ingestion: Read and Union Trip Data ---\n",
    "\n",
    "print(\"Starting ingestion and union of 8 months of trip data...\")\n",
    "\n",
    "\n",
    "raw_trips_df = (spark.read\n",
    "                .format(\"parquet\")\n",
    "                .load(TRIP_DATA_PATHS)\n",
    "               )\n",
    "\n",
    "# Verifying the load by counting and displaying a sample\n",
    "total_records = raw_trips_df.count()\n",
    "print(f\"Total raw records loaded: {total_records}\")\n",
    "\n",
    "print(\"\\nRaw Data Schema:\")\n",
    "# Print the data types to ensure they were inferred correctly\n",
    "raw_trips_df.printSchema()\n",
    "\n",
    "print(\"\\nRaw Data Sample:\")\n",
    "raw_trips_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721563ed-ed32-4d42-8710-4898de41a07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting initial cleaning and filtering...\")\n",
    "\n",
    "# Filtering out records with invalid or impossible values ensures the integrity of all subsequent analytical calculations (e.g., averages, sums, and ML model training)\n",
    "\n",
    "cleaned_trips_df = (raw_trips_df\n",
    "    # TRIP DISTANCE FILTER\n",
    "    # A trip distance of 0 is either an error or a canceled ride where the meter was not reset.\n",
    "    # I am exclduing these records from the dataset as they create infinite speed values (distance/time) and skew metrics.\n",
    "    .filter(col(\"trip_distance\") > 0)\n",
    "    \n",
    "    # FARE AMOUNT FILTER\n",
    "    # A fare of less than $1 suggests a free trip, a severe error, or a test transaction.\n",
    "    .filter(col(\"fare_amount\") >= 1.0)\n",
    "    \n",
    "    # PASSENGER COUNT FILTER\n",
    "    # A passenger count of 0 is illogical for a completed trip and usually indicates a data entry error.\n",
    "    .filter(col(\"passenger_count\") > 0)\n",
    "    \n",
    "    # 4. PAYMENT TYPE FILTER\n",
    "    # Focusing on payment types 1 (Credit Card) and 2 (Cash) provides the most reliable data\n",
    "    # for revenue analysis. Types 3 (No Charge) and 4 (Dispute) are typically excluded from core revenue KPIs.\n",
    "    .filter(col(\"payment_type\").isin([1, 2])) \n",
    "    \n",
    "    # COLUMN RENAMING\n",
    "    # Renaming columns to a cleaner, more generalized standard to make the code easier to read\n",
    "    # and maintain, as 'tpep_' prefixes are specific to \"Yellow Taxi\" data.\n",
    "    .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "    .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    ")\n",
    "\n",
    "# Check removed rows\n",
    "initial_records = raw_trips_df.count()\n",
    "final_records = cleaned_trips_df.count()\n",
    "print(f\"Initial Records: {initial_records}\")\n",
    "print(f\"Records after cleaning: {final_records}\")\n",
    "print(f\"Records removed: {initial_records - final_records}\")\n",
    "\n",
    "# Display schema \n",
    "print(\"\\nCleaned Data Schema:\")\n",
    "cleaned_trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2445aa-d946-4841-a962-ebaf7ab4c805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting feature engineering...\")\n",
    "\n",
    "\n",
    "transformed_df = (cleaned_trips_df\n",
    "    # TRIP DURATION CALCULATION\n",
    "    # Feature: 'trip_duration_seconds'\n",
    "    # This feature is essential for time-based analysis (e.g., identifying high-traffic hours) and is necessary to calculate speed. PySpark `unix_timestamp` function for fast, distributed calculation.\n",
    "    .withColumn(\n",
    "        \"trip_duration_seconds\",\n",
    "        unix_timestamp(col(\"dropoff_datetime\")) - unix_timestamp(col(\"pickup_datetime\"))\n",
    "    )\n",
    "    \n",
    "    # AVERAGE SPEED CALCULATION\n",
    "    # Feature: 'avg_speed_mph'\n",
    "    # Average speed is a powerful metric for analyzing city congestion, driver efficiency, and identifying potential outliers (like trips with impossible speeds).\n",
    "    .withColumn(\n",
    "        \"avg_speed_mph\",\n",
    "        # Formula: distance / (duration_in_seconds / 3600 seconds per hour)\n",
    "        col(\"trip_distance\") / (col(\"trip_duration_seconds\") / 3600)\n",
    "    )\n",
    "    \n",
    "    #  FINAL DURATION FILTER\n",
    "    # A trip lasting less than 60 seconds is physically unlikely for the NYC system. \n",
    "    # This filter removes more noise, ensuring we focus on legitimate completed trips.\n",
    "    .filter(col(\"trip_duration_seconds\") > 60) \n",
    "    \n",
    "    # FINAL SPEED FILTER\n",
    "    # Speeds over 60 MPH are nearly impossible within city limits. This acts as a final layer of quality control, identifying and removing trips that are likely GPS errors or data corruption.\n",
    "    .filter(col(\"avg_speed_mph\") <= 60) \n",
    "    \n",
    "    # SELECT AND ORDER COLUMNS\n",
    "    # Explicitly selecting columns ensures only relevant fields are carried forward, improving performance (by reducing data size) and providing a clean output schema for the next phase.\n",
    "    .select(\n",
    "        \"VendorID\", \"pickup_datetime\", \"dropoff_datetime\", \n",
    "        \"PULocationID\", \"DOLocationID\", \"passenger_count\",\n",
    "        \"trip_distance\", \"fare_amount\", \"tip_amount\", \n",
    "        \"total_amount\", \"payment_type\", \n",
    "        \"trip_duration_seconds\", \"avg_speed_mph\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Final verification of the total count after all filters\n",
    "print(f\"Final records after feature engineering and final speed filter: {transformed_df.count()}\")\n",
    "transformed_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69137425-715f-498e-b76c-1902f30fddf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting ingestion of the small lookup table...\")\n",
    "\n",
    "\n",
    "# Read the CSV file (lookup table witj zip codes)\n",
    "lookup_df = (spark.read\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", \"true\") #The first row is the header\n",
    "             .option(\"inferSchema\", \"true\") # guess the correct data types\n",
    "             .load(LOOKUP_TABLE_PATH)\n",
    "            )\n",
    "\n",
    "print(\"\\nLookup Table Schema:\")\n",
    "lookup_df.printSchema()\n",
    "\n",
    "print(\"\\nLookup Table Sample:\")\n",
    "lookup_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8aac1a-2539-4aa3-bfc1-85fef7153f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nJoining trip data with the lookup table...\")\n",
    "\n",
    "# This step creates the final, ready-for-analysis dataset.\n",
    "\n",
    "# Create the final joined DataFrame\n",
    "joined_df = (transformed_df\n",
    "    # Join the main trip data with the lookup table on the Pickup Location ID (PULocationID)\n",
    "    .join(\n",
    "        lookup_df, \n",
    "        # Define the join condition: Trip's pickup ID must match the Lookup table's ID\n",
    "        (transformed_df[\"PULocationID\"] == lookup_df[\"LocationID\"]), \n",
    "        \"inner\" # Inner join to ensure every trip has a valid, known zone name\n",
    "    )\n",
    "    .select(\n",
    "        transformed_df[\"*\"], # Keep all columns from the trip data\n",
    "        lookup_df[\"Zone\"].alias(\"Pickup_Zone\"),\n",
    "        lookup_df[\"Borough\"].alias(\"Pickup_Borough\")\n",
    "    )\n",
    "    # Drop the original PULocationID column as the zone name is more useful now\n",
    "    .drop(\"PULocationID\")\n",
    ")\n",
    "\n",
    "# Display a sample of the joined data \n",
    "print(f\"Total records after join: {joined_df.count()}\")\n",
    "print(\"\\nJoined Data Sample:\")\n",
    "joined_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8380ecfb-8513-4dac-84a4-c8104826b008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase_1_Ingestion_and_Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7908f475-93f0-46bd-a162-0e4a2d68fa4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# List of paths for the large monthly trip data files (Parquet).\n",
    "TRIP_DATA_PATHS = [\n",
    "    f\"/Volumes/nyc_yellow_tripdata/default/filestore/yellow_tripdata_2025-{month:02d}.parquet\" \n",
    "    for month in range(1, 9) # This generates months 01-08\n",
    "]\n",
    "\n",
    "# Path for the Taxi Zone Lookup CSV\n",
    "LOOKUP_TABLE_PATH = \"/Volumes/nyc_yellow_tripdata/default/filestore/taxi_zone_lookup.csv\"\n",
    "\n",
    "\n",
    "# --- Ingestion: Read and Union Trip Data ---\n",
    "\n",
    "print(\"Starting ingestion and union of 8 months of trip data...\")\n",
    "\n",
    "\n",
    "raw_trips_df = (spark.read\n",
    "                .format(\"parquet\")\n",
    "                .load(TRIP_DATA_PATHS)\n",
    "               )\n",
    "\n",
    "# Verifying the load by counting and displaying a sample\n",
    "total_records = raw_trips_df.count()\n",
    "print(f\"Total raw records loaded: {total_records}\")\n",
    "\n",
    "print(\"\\nRaw Data Schema:\")\n",
    "# Print the data types to ensure they were inferred correctly\n",
    "raw_trips_df.printSchema()\n",
    "\n",
    "print(\"\\nRaw Data Sample:\")\n",
    "raw_trips_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721563ed-ed32-4d42-8710-4898de41a07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting initial cleaning and filtering...\")\n",
    "\n",
    "# Filtering out records with invalid or impossible values ensures the integrity of all subsequent analytical calculations (e.g., averages, sums, and ML model training)\n",
    "\n",
    "cleaned_trips_df = (raw_trips_df\n",
    "    # TRIP DISTANCE FILTER\n",
    "    # A trip distance of 0 is either an error or a canceled ride where the meter was not reset.\n",
    "    # I am exclduing these records from the dataset as they create infinite speed values (distance/time) and skew metrics.\n",
    "    .filter(col(\"trip_distance\") > 0)\n",
    "    \n",
    "    # FARE AMOUNT FILTER\n",
    "    # A fare of less than $1 suggests a free trip, a severe error, or a test transaction.\n",
    "    .filter(col(\"fare_amount\") >= 1.0)\n",
    "    \n",
    "    # PASSENGER COUNT FILTER\n",
    "    # A passenger count of 0 is illogical for a completed trip and usually indicates a data entry error.\n",
    "    .filter(col(\"passenger_count\") > 0)\n",
    "    \n",
    "    # 4. PAYMENT TYPE FILTER\n",
    "    # Focusing on payment types 1 (Credit Card) and 2 (Cash) provides the most reliable data\n",
    "    # for revenue analysis. Types 3 (No Charge) and 4 (Dispute) are typically excluded from core revenue KPIs.\n",
    "    .filter(col(\"payment_type\").isin([1, 2])) \n",
    "    \n",
    "    # COLUMN RENAMING\n",
    "    # Renaming columns to a cleaner, more generalized standard to make the code easier to read\n",
    "    # and maintain, as 'tpep_' prefixes are specific to \"Yellow Taxi\" data.\n",
    "    .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "    .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    ")\n",
    "\n",
    "# Check removed rows\n",
    "initial_records = raw_trips_df.count()\n",
    "final_records = cleaned_trips_df.count()\n",
    "print(f\"Initial Records: {initial_records}\")\n",
    "print(f\"Records after cleaning: {final_records}\")\n",
    "print(f\"Records removed: {initial_records - final_records}\")\n",
    "\n",
    "# Display schema \n",
    "print(\"\\nCleaned Data Schema:\")\n",
    "cleaned_trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2445aa-d946-4841-a962-ebaf7ab4c805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting feature engineering...\")\n",
    "\n",
    "\n",
    "transformed_df = (cleaned_trips_df\n",
    "    # TRIP DURATION CALCULATION\n",
    "    # Feature: 'trip_duration_seconds'\n",
    "    # This feature is essential for time-based analysis (e.g., identifying high-traffic hours) and is necessary to calculate speed. PySpark `unix_timestamp` function for fast, distributed calculation.\n",
    "    .withColumn(\n",
    "        \"trip_duration_seconds\",\n",
    "        unix_timestamp(col(\"dropoff_datetime\")) - unix_timestamp(col(\"pickup_datetime\"))\n",
    "    )\n",
    "    \n",
    "    # AVERAGE SPEED CALCULATION\n",
    "    # Feature: 'avg_speed_mph'\n",
    "    # Average speed is a powerful metric for analyzing city congestion, driver efficiency, and identifying potential outliers (like trips with impossible speeds).\n",
    "    .withColumn(\n",
    "        \"avg_speed_mph\",\n",
    "        # Formula: distance / (duration_in_seconds / 3600 seconds per hour)\n",
    "        col(\"trip_distance\") / (col(\"trip_duration_seconds\") / 3600)\n",
    "    )\n",
    "    \n",
    "    #  FINAL DURATION FILTER\n",
    "    # A trip lasting less than 60 seconds is physically unlikely for the NYC system. \n",
    "    # This filter removes more noise, ensuring we focus on legitimate completed trips.\n",
    "    .filter(col(\"trip_duration_seconds\") > 60) \n",
    "    \n",
    "    # FINAL SPEED FILTER\n",
    "    # Speeds over 60 MPH are nearly impossible within city limits. This acts as a final layer of quality control, identifying and removing trips that are likely GPS errors or data corruption.\n",
    "    .filter(col(\"avg_speed_mph\") <= 60) \n",
    "    \n",
    "    # SELECT AND ORDER COLUMNS\n",
    "    # Explicitly selecting columns ensures only relevant fields are carried forward, improving performance (by reducing data size) and providing a clean output schema for the next phase.\n",
    "    .select(\n",
    "        \"VendorID\", \"pickup_datetime\", \"dropoff_datetime\", \n",
    "        \"PULocationID\", \"DOLocationID\", \"passenger_count\",\n",
    "        \"trip_distance\", \"fare_amount\", \"tip_amount\", \n",
    "        \"total_amount\", \"payment_type\", \n",
    "        \"trip_duration_seconds\", \"avg_speed_mph\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Final verification of the total count after all filters\n",
    "print(f\"Final records after feature engineering and final speed filter: {transformed_df.count()}\")\n",
    "transformed_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69137425-715f-498e-b76c-1902f30fddf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting ingestion of the small lookup table...\")\n",
    "\n",
    "\n",
    "# Read the CSV file (lookup table witj zip codes)\n",
    "lookup_df = (spark.read\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", \"true\") #The first row is the header\n",
    "             .option(\"inferSchema\", \"true\") # guess the correct data types\n",
    "             .load(LOOKUP_TABLE_PATH)\n",
    "            )\n",
    "\n",
    "print(\"\\nLookup Table Schema:\")\n",
    "lookup_df.printSchema()\n",
    "\n",
    "print(\"\\nLookup Table Sample:\")\n",
    "lookup_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8aac1a-2539-4aa3-bfc1-85fef7153f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nJoining trip data with the lookup table...\")\n",
    "\n",
    "# This step creates the final, ready-for-analysis dataset.\n",
    "\n",
    "# Create the final joined DataFrame\n",
    "joined_df = (transformed_df\n",
    "    # Join the main trip data with the lookup table on the Pickup Location ID (PULocationID)\n",
    "    .join(\n",
    "        lookup_df, \n",
    "        # Define the join condition: Trip's pickup ID must match the Lookup table's ID\n",
    "        (transformed_df[\"PULocationID\"] == lookup_df[\"LocationID\"]), \n",
    "        \"inner\" # Inner join to ensure every trip has a valid, known zone name\n",
    "    )\n",
    "    .select(\n",
    "        transformed_df[\"*\"], # Keep all columns from the trip data\n",
    "        lookup_df[\"Zone\"].alias(\"Pickup_Zone\"),\n",
    "        lookup_df[\"Borough\"].alias(\"Pickup_Borough\")\n",
    "    )\n",
    "    # Drop the original PULocationID column as the zone name is more useful now\n",
    "    .drop(\"PULocationID\")\n",
    ")\n",
    "\n",
    "# Display a sample of the joined data \n",
    "print(f\"Total records after join: {joined_df.count()}\")\n",
    "print(\"\\nJoined Data Sample:\")\n",
    "joined_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8380ecfb-8513-4dac-84a4-c8104826b008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting calculation of 7-day Rolling Average Fare...\")\n",
    "\n",
    "# --- Feature Engineering: Create Date Columns ---\n",
    "# Extracting the date (year, month, day) from the timestampfor grouping, partitioning,and ordering the data within the Window Function.\n",
    "dated_df = (joined_df\n",
    "    .withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\n",
    "    .withColumn(\"pickup_year\", year(col(\"pickup_datetime\")))\n",
    "    .withColumn(\"pickup_month\", month(col(\"pickup_datetime\")))\n",
    ")\n",
    "\n",
    "# --- Define the Window Specification ---\n",
    "# --- Partions ---\n",
    "# Partition By: 'Pickup_Zone' to ensure the rolling average calculation restarts for every zone.\n",
    "# Order By: 'pickup_date' to ensure the calculation is applied chronologically.\n",
    "# Rows Between: '-7' (7 days before) and '0' (the current day) defines the 7-day rolling window.\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "zone_daily_window = (Window\n",
    "    .partitionBy(\"Pickup_Zone\")\n",
    "    .orderBy(\"pickup_date\")\n",
    "    .rowsBetween(-7, 0) \n",
    ")\n",
    "\n",
    "# --- Apply the Window Function ---\n",
    "rolling_avg_df = (dated_df\n",
    "    .withColumn(\n",
    "        \"7_day_rolling_avg_fare\",\n",
    "        # Calculate the average fare amount over the defined 7-day window\n",
    "        avg(\"fare_amount\").over(zone_daily_window)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"7_day_rolling_avg_speed\",\n",
    "        # Calculate the average speed over the same window\n",
    "        avg(\"avg_speed_mph\").over(zone_daily_window)\n",
    "    )\n",
    "    .orderBy(\"Pickup_Zone\", \"pickup_date\") # Order for clear visualization\n",
    ")\n",
    "\n",
    "print(\"\\nRolling Average Calculation Complete. Sample data:\")\n",
    "rolling_avg_df.select(\n",
    "    \"Pickup_Zone\", \"pickup_date\", \"fare_amount\", \"7_day_rolling_avg_fare\", \"7_day_rolling_avg_speed\"\n",
    ").limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a0f612-8570-4b50-9c9b-b6415c691226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting Daily Revenue and Trip Count Aggregation...\")\n",
    "\n",
    "# --- Group and Aggregate ---\n",
    "\n",
    "daily_summary_df = (rolling_avg_df\n",
    "    .groupBy(\"pickup_date\", \"Pickup_Borough\") # Group by date and borough to see daily performance per area\n",
    "    .agg(\n",
    "        # Calculate the total fare revenue for the day\n",
    "        sum(\"fare_amount\").alias(\"Total_Daily_Revenue\"),\n",
    "        # Count the total number of trips\n",
    "        count(lit(1)).alias(\"Total_Trips\"),\n",
    "        # Calculate the overall average speed for the day\n",
    "        avg(\"avg_speed_mph\").alias(\"Overall_Daily_Avg_Speed\")\n",
    "    )\n",
    "    .orderBy(\"pickup_date\", \"Pickup_Borough\")\n",
    ")\n",
    "\n",
    "print(\"\\nDaily Summary Aggregation Complete. Sample data:\")\n",
    "daily_summary_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88626e03-ae46-4fcf-8b4c-6bcec8f40ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Initial Load (L) to Delta Lake ---\n",
    "TARGET_TABLE_NAME = \"nyc_taxi_analysis_baseline\"\n",
    "\n",
    "print(f\"\\nStarting Initial Load to Delta Table: {TARGET_TABLE_NAME}...\")\n",
    "\n",
    "# Partitioning by Year/Month to optimize data skipping during future queries.\n",
    "\n",
    "(rolling_avg_df\n",
    "    .write\n",
    "    .mode(\"overwrite\") # Overwrite the table each time the notebook runs\n",
    "    .format(\"delta\")\n",
    "    #  Partitioning by Year and Month to optimize data skipping, to make queries much faster.\n",
    "    .partitionBy(\"pickup_year\", \"pickup_month\") \n",
    "    .saveAsTable(TARGET_TABLE_NAME) # Save the data as a managed table in Unity Catalog\n",
    ")\n",
    "\n",
    "print(f\"Initial Load Complete. Baseline table '{TARGET_TABLE_NAME}' created and partitioned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a2a820-9587-494f-82b4-0c26e85906ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  --- Broadcast Join ---\n",
    "# Force Spark to send the entire small table to every worker node, eliminating the need \n",
    "# to shuffle the massive trip data, which will drastically reduces network overhead and execution time.\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "print(\"\\nStarting Optimized Join (Broadcast Join)...\")\n",
    "\n",
    "# --- Optimized Join ---\n",
    "optimized_joined_df = (transformed_df\n",
    "    # Apply the broadcast hint to the smaller DataFrame (lookup_df)\n",
    "    .join(\n",
    "        broadcast(lookup_df), \n",
    "        (transformed_df[\"PULocationID\"] == lookup_df[\"LocationID\"]), \n",
    "        \"inner\"\n",
    "    )\n",
    "    # Select and rename columns, identical to the previous join step\n",
    "    .select(\n",
    "        transformed_df[\"*\"],\n",
    "        lookup_df[\"Zone\"].alias(\"Pickup_Zone_Optimized\"), # Rename for comparison\n",
    "        lookup_df[\"Borough\"].alias(\"Pickup_Borough_Optimized\")\n",
    "    )\n",
    "    .drop(\"PULocationID\")\n",
    ")\n",
    "\n",
    "optimized_joined_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd7f5800-8c13-4966-9daa-6bf7a835fdb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This re-uses the Window definition calculated above\n",
    "from pyspark.sql.window import Window\n",
    "zone_daily_window = (Window.partitionBy(\"Pickup_Zone_Optimized\").orderBy(\"pickup_date\").rowsBetween(-7, 0))\n",
    "\n",
    "optimized_final_df = (optimized_joined_df\n",
    "    .withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\n",
    "    .withColumn(\"pickup_year\", year(col(\"pickup_datetime\")))\n",
    "    .withColumn(\"pickup_month\", month(col(\"pickup_datetime\")))\n",
    "    .withColumn(\n",
    "        \"7_day_rolling_avg_fare_opt\",\n",
    "        avg(\"fare_amount\").over(zone_daily_window)\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Final Load (L) to Delta Lake - OPTIMIZED ---\n",
    "\n",
    "TARGET_TABLE_NAME_OPT = \"nyc_taxi_analysis_optimized\"\n",
    "\n",
    "print(f\"\\nStarting Final Load to Optimized Delta Table: {TARGET_TABLE_NAME_OPT}...\")\n",
    "\n",
    "# Write the fully optimized pipeline output\n",
    "(optimized_final_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"pickup_year\", \"pickup_month\") \n",
    "    .saveAsTable(TARGET_TABLE_NAME_OPT)\n",
    ")\n",
    "\n",
    "print(f\"Optimized Load Complete. Table '{TARGET_TABLE_NAME_OPT}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a091b65-b2f4-486c-a335-d2a2c837d411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase_1_Ingestion_and_Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
